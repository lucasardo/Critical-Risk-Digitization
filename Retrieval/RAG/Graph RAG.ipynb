{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from __future__ import annotations\n",
    "    \n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_openai import AzureOpenAIEmbeddings    \n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_core.callbacks.manager import CallbackManager, CallbackManagerForChainRun\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain_core.prompts.base import BasePromptTemplate\n",
    "from pydantic import Field\n",
    "\n",
    "from langchain_community.chains.graph_qa.prompts import (\n",
    "    GRAPHDB_QA_PROMPT,\n",
    "    GRAPHDB_SPARQL_FIX_PROMPT,\n",
    "    GRAPHDB_SPARQL_GENERATION_PROMPT,\n",
    ")\n",
    "from langchain_community.graphs import OntotextGraphDBGraph\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from langsmith import Client\n",
    "from langsmith import traceable\n",
    "\n",
    "from langchain_community.graphs import OntotextGraphDBGraph\n",
    "from typing import TYPE_CHECKING, Any, Dict, List, Optional\n",
    "if TYPE_CHECKING:\n",
    "    import rdflib\n",
    "\n",
    "from rdflib.plugins.sparql import prepareQuery\n",
    "from rdflib import Graph\n",
    "from rdflib import Namespace\n",
    "    \n",
    "import textwrap\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(os.path.join(\"..\", \"Azure OpenAI credentials.env\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AZURESEARCH_FIELDS_ID\"] = \"id\"\n",
    "os.environ[\"AZURESEARCH_FIELDS_CONTENT\"] = \"chunk\"\n",
    "os.environ[\"AZURESEARCH_FIELDS_CONTENT_VECTOR\"] = \"embedding\"\n",
    "\n",
    "from langchain.vectorstores import AzureSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_endpoint = os.environ['GLOBAL_AZURE_ENDPOINT']\n",
    "openai_api_key = os.environ['GLOBAL_OPENAI_API_KEY']\n",
    "\n",
    "openai_deployment_name = os.environ['GLOBAL_GPT_DEPLOYMENT_NAME']\n",
    "openai_api_version = os.environ['GLOBAL_OPENAI_API_VERSION']\n",
    "embedding_model = os.environ['GLOBAL_EMBEDDING_MODEL']\n",
    "embedding_deployment_name = os.environ['GLOBAL_EMBEDDING_DEPLOYMENT_NAME']\n",
    "\n",
    "search_endpoint = os.environ['SEARCH_ENDPOINT']\n",
    "search_api_key = os.environ['SEARCH_API_KEY']\n",
    "search_api_version = os.environ['SEARCH_API_VERSION']\n",
    "search_service_name = os.environ['SEARCH_SERVICE_NAME']\n",
    "\n",
    "# langsmith_api_key = os.environ['LANGSMITH_API_KEY']\n",
    "\n",
    "search_url = f\"https://{search_service_name}.search.windows.net/\"\n",
    "search_credential = AzureKeyCredential(search_api_key)\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=openai_deployment_name, \n",
    "    openai_api_version=openai_api_version, \n",
    "    openai_api_key=openai_api_key, \n",
    "    azure_endpoint=azure_endpoint, \n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=embedding_deployment_name,\n",
    "    api_version=openai_api_version,\n",
    "    api_key=openai_api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    ")\n",
    "\n",
    "index_name: str = \"crd-vector-store\"\n",
    "ontology_index_name: str = \"crd-vector-store-ontologies\"\n",
    "\n",
    "vector_store: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint=search_endpoint,\n",
    "    azure_search_key=search_api_key,\n",
    "    index_name=index_name,\n",
    "    embedding_function=embeddings.embed_query,\n",
    ")\n",
    "\n",
    "ontology_vector_store: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint=search_endpoint,\n",
    "    azure_search_key=search_api_key,\n",
    "    index_name=ontology_index_name,\n",
    "    embedding_function=embeddings.embed_query,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph database config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#dateTime, Converter=<function parse_datetime at 0x0000026ABB8198A0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\isodate\\isodatetime.py\", line 51, in parse_datetime\n",
      "    datestring, timestring = datetimestring.split('T')\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: not enough values to unpack (expected 2, got 1)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\isodate\\isodatetime.py\", line 53, in parse_datetime\n",
      "    raise ISO8601Error(\"ISO 8601 time designator 'T' missing. Unable to\"\n",
      "isodate.isoerror.ISO8601Error: ISO 8601 time designator 'T' missing. Unable to parse datetime string '2024-09-24'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#dateTime, Converter=<function parse_datetime at 0x0000026ABB8198A0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\isodate\\isodatetime.py\", line 51, in parse_datetime\n",
      "    datestring, timestring = datetimestring.split('T')\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: not enough values to unpack (expected 2, got 1)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\isodate\\isodatetime.py\", line 53, in parse_datetime\n",
      "    raise ISO8601Error(\"ISO 8601 time designator 'T' missing. Unable to\"\n",
      "isodate.isoerror.ISO8601Error: ISO 8601 time designator 'T' missing. Unable to parse datetime string '2024-09-24'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#dateTime, Converter=<function parse_datetime at 0x0000026ABB8198A0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\isodate\\isodatetime.py\", line 51, in parse_datetime\n",
      "    datestring, timestring = datetimestring.split('T')\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: not enough values to unpack (expected 2, got 1)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\isodate\\isodatetime.py\", line 53, in parse_datetime\n",
      "    raise ISO8601Error(\"ISO 8601 time designator 'T' missing. Unable to\"\n",
      "isodate.isoerror.ISO8601Error: ISO 8601 time designator 'T' missing. Unable to parse datetime string '2024-09-24'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#dateTime, Converter=<function parse_datetime at 0x0000026ABB8198A0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\isodate\\isodatetime.py\", line 51, in parse_datetime\n",
      "    datestring, timestring = datetimestring.split('T')\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: not enough values to unpack (expected 2, got 1)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\isodate\\isodatetime.py\", line 53, in parse_datetime\n",
      "    raise ISO8601Error(\"ISO 8601 time designator 'T' missing. Unable to\"\n",
      "isodate.isoerror.ISO8601Error: ISO 8601 time designator 'T' missing. Unable to parse datetime string '2024-09-24'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#dateTime, Converter=<function parse_datetime at 0x0000026ABB8198A0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\isodate\\isodatetime.py\", line 51, in parse_datetime\n",
      "    datestring, timestring = datetimestring.split('T')\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: not enough values to unpack (expected 2, got 1)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\isodate\\isodatetime.py\", line 53, in parse_datetime\n",
      "    raise ISO8601Error(\"ISO 8601 time designator 'T' missing. Unable to\"\n",
      "isodate.isoerror.ISO8601Error: ISO 8601 time designator 'T' missing. Unable to parse datetime string '2024-09-24'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#dateTime, Converter=<function parse_datetime at 0x0000026ABB8198A0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\isodate\\isodatetime.py\", line 51, in parse_datetime\n",
      "    datestring, timestring = datetimestring.split('T')\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: not enough values to unpack (expected 2, got 1)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\isodate\\isodatetime.py\", line 53, in parse_datetime\n",
      "    raise ISO8601Error(\"ISO 8601 time designator 'T' missing. Unable to\"\n",
      "isodate.isoerror.ISO8601Error: ISO 8601 time designator 'T' missing. Unable to parse datetime string '2023-08-22'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#dateTime, Converter=<function parse_datetime at 0x0000026ABB8198A0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\isodate\\isodatetime.py\", line 51, in parse_datetime\n",
      "    datestring, timestring = datetimestring.split('T')\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: not enough values to unpack (expected 2, got 1)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\isodate\\isodatetime.py\", line 53, in parse_datetime\n",
      "    raise ISO8601Error(\"ISO 8601 time designator 'T' missing. Unable to\"\n",
      "isodate.isoerror.ISO8601Error: ISO 8601 time designator 'T' missing. Unable to parse datetime string '2023-09-01'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#dateTime, Converter=<function parse_datetime at 0x0000026ABB8198A0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\isodate\\isodatetime.py\", line 51, in parse_datetime\n",
      "    datestring, timestring = datetimestring.split('T')\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: not enough values to unpack (expected 2, got 1)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\isodate\\isodatetime.py\", line 53, in parse_datetime\n",
      "    raise ISO8601Error(\"ISO 8601 time designator 'T' missing. Unable to\"\n",
      "isodate.isoerror.ISO8601Error: ISO 8601 time designator 'T' missing. Unable to parse datetime string '2023'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#dateTime, Converter=<function parse_datetime at 0x0000026ABB8198A0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\isodate\\isodatetime.py\", line 51, in parse_datetime\n",
      "    datestring, timestring = datetimestring.split('T')\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: not enough values to unpack (expected 2, got 1)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\isodate\\isodatetime.py\", line 53, in parse_datetime\n",
      "    raise ISO8601Error(\"ISO 8601 time designator 'T' missing. Unable to\"\n",
      "isodate.isoerror.ISO8601Error: ISO 8601 time designator 'T' missing. Unable to parse datetime string '2023-08-22 to 2023-09-01'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#dateTime, Converter=<function parse_datetime at 0x0000026ABB8198A0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\isodate\\isodatetime.py\", line 51, in parse_datetime\n",
      "    datestring, timestring = datetimestring.split('T')\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: not enough values to unpack (expected 2, got 1)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\isodate\\isodatetime.py\", line 53, in parse_datetime\n",
      "    raise ISO8601Error(\"ISO 8601 time designator 'T' missing. Unable to\"\n",
      "isodate.isoerror.ISO8601Error: ISO 8601 time designator 'T' missing. Unable to parse datetime string '2023-08-22'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#dateTime, Converter=<function parse_datetime at 0x0000026ABB8198A0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\isodate\\isodatetime.py\", line 51, in parse_datetime\n",
      "    datestring, timestring = datetimestring.split('T')\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: not enough values to unpack (expected 2, got 1)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\isodate\\isodatetime.py\", line 53, in parse_datetime\n",
      "    raise ISO8601Error(\"ISO 8601 time designator 'T' missing. Unable to\"\n",
      "isodate.isoerror.ISO8601Error: ISO 8601 time designator 'T' missing. Unable to parse datetime string '2023-09-01'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#dateTime, Converter=<function parse_datetime at 0x0000026ABB8198A0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\isodate\\isodatetime.py\", line 51, in parse_datetime\n",
      "    datestring, timestring = datetimestring.split('T')\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: not enough values to unpack (expected 2, got 1)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\isodate\\isodatetime.py\", line 53, in parse_datetime\n",
      "    raise ISO8601Error(\"ISO 8601 time designator 'T' missing. Unable to\"\n",
      "isodate.isoerror.ISO8601Error: ISO 8601 time designator 'T' missing. Unable to parse datetime string '2023-09-21'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#dateTime, Converter=<function parse_datetime at 0x0000026ABB8198A0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\isodate\\isodatetime.py\", line 51, in parse_datetime\n",
      "    datestring, timestring = datetimestring.split('T')\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: not enough values to unpack (expected 2, got 1)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\isodate\\isodatetime.py\", line 53, in parse_datetime\n",
      "    raise ISO8601Error(\"ISO 8601 time designator 'T' missing. Unable to\"\n",
      "isodate.isoerror.ISO8601Error: ISO 8601 time designator 'T' missing. Unable to parse datetime string '2023-10-07'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#dateTime, Converter=<function parse_datetime at 0x0000026ABB8198A0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\isodate\\isodatetime.py\", line 51, in parse_datetime\n",
      "    datestring, timestring = datetimestring.split('T')\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: not enough values to unpack (expected 2, got 1)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ITLS104415\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\isodate\\isodatetime.py\", line 53, in parse_datetime\n",
      "    raise ISO8601Error(\"ISO 8601 time designator 'T' missing. Unable to\"\n",
      "isodate.isoerror.ISO8601Error: ISO 8601 time designator 'T' missing. Unable to parse datetime string '2023-10-16'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Graph identifier=Ncbc1113f11f24ae8ba5b685db96b115a (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ontology_path = os.path.join(\"..\", \"ontology\", \"CRA V15.3.ttl\")\n",
    "inst_ontology_path = os.path.join(\"..\", \"ontology\", \"CRA V15.3.ttl\")\n",
    "g = Graph()\n",
    "g.parse(inst_ontology_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicate: http://www.w3.org/1999/02/22-rdf-syntax-ns#type, Object: http://www.w3.org/2002/07/owl#NamedIndividual\n",
      "Predicate: http://www.w3.org/1999/02/22-rdf-syntax-ns#type, Object: http://WSP.org/ontology/cro#Operation\n",
      "Predicate: http://www.w3.org/1999/02/22-rdf-syntax-ns#type, Object: http://WSP.org/ontology/cro#Port\n",
      "Predicate: http://WSP.org/ontology/cro#hasProject, Object: http://WSP.org/ontology/cro#DPO_Dampier_Resilience_Project\n",
      "Predicate: http://WSP.org/ontology/cro#hasRiskAsessmentReport, Object: http://WSP.org/ontology/cro#DPO_CriticalRiskAassesmentReport_2023\n",
      "Predicate: http://WSP.org/ontology/cro#hasTerminal, Object: http://WSP.org/ontology/cro#DPO_East_Intercourse_Island\n",
      "Predicate: http://WSP.org/ontology/cro#hasTerminal, Object: http://WSP.org/ontology/cro#DPO_Parker_Point\n",
      "Predicate: http://WSP.org/ontology/cro#hasAddress, Object: PO Box 21, 6713 Dampier WA, AU\n",
      "Predicate: http://WSP.org/ontology/cro#hasCRAType, Object: MFL\n",
      "Predicate: http://WSP.org/ontology/cro#hasDateOfSurvey, Object: 2023-08-22\n",
      "Predicate: http://WSP.org/ontology/cro#hasDateOfSurvey, Object: 2023-09-01\n",
      "Predicate: http://WSP.org/ontology/cro#hasLatitude, Object: -20.6616\n",
      "Predicate: http://WSP.org/ontology/cro#hasLongitude, Object: 116.7071\n",
      "Predicate: http://WSP.org/ontology/cro#hasName, Object: Dampier Port Operations\n",
      "Predicate: http://WSP.org/ontology/cro#hasPhone, Object: +61 (0) 8 9183 7004\n",
      "Predicate: http://WSP.org/ontology/cro#numberOfContractors, Object: 50\n",
      "Predicate: http://WSP.org/ontology/cro#numberOfEmployees, Object: 200\n",
      "Predicate: http://WSP.org/ontology/cro#portName, Object: Dampier Port\n"
     ]
    }
   ],
   "source": [
    "# Graph test\n",
    "\n",
    "# Define namespaces\n",
    "CRO = Namespace(\"http://WSP.org/ontology/cro#\")\n",
    "XSD = Namespace(\"http://www.w3.org/2001/XMLSchema#\")\n",
    "\n",
    "# Bind the namespaces to the graph (if not already bound)\n",
    "g.bind(\"cro\", CRO)\n",
    "g.bind(\"xsd\", XSD)\n",
    "\n",
    "# Define the SPARQL query to retrieve all predicates and objects for DPO_Operation\n",
    "cro_query = \"\"\"\n",
    "PREFIX cro: <http://WSP.org/ontology/cro#>\n",
    "PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "\n",
    "SELECT ?predicate ?object\n",
    "WHERE {\n",
    "    cro:DPO_Operation ?predicate ?object .\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Run the query\n",
    "results = g.query(cro_query)\n",
    "\n",
    "# Iterate over the results and print\n",
    "for row in results:\n",
    "    print(f\"Predicate: {row.predicate}, Object: {row.object}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ontology_path, 'r', encoding='utf-8') as file:\n",
    "    ontology = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "individuals_start_string = \"#    Individuals\"\n",
    "ontology = ontology[:ontology.find(individuals_start_string)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sparql_retries = 5  # Max number of attempts at generating a correct SPARQL query\n",
    "max_query_retries = 3   # Max number of attempts at generating a SPARQL query that returns at least one result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARQL_SELECT_TEMPLATE = \"\"\"\n",
    "Task: Generate a SPARQL SELECT statement for querying a graph database.\n",
    "\n",
    "For instance, a question could be: \"List all the Critical Risk Scenarios that impact 'Dampier Port'\".\n",
    "\n",
    "The following sparql query in backticks would be suitable:\n",
    "```\n",
    "PREFIX cro: <http://WSP.org/ontology/cro#>\n",
    "SELECT ?criticalRiskScenario\n",
    "WHERE {{\n",
    "    ?operation cro:hasName \"Dampier Port Operations\" .\n",
    "    ?criticalRiskScenario cro:impactsOperation ?operation .\n",
    "}}\n",
    "```\n",
    "Instructions:\n",
    "Use only the node types and properties provided in the schema.\n",
    "Do not use any node types and properties that are not explicitly provided.\n",
    "Do not wrap the query in backticks.\n",
    "Include all necessary prefixes.\n",
    "Schema:\n",
    "{schema}\n",
    "Note: Be as concise as possible.\n",
    "Do not include any explanations or apologies in your responses.\n",
    "Do not respond to any questions that ask for anything else than for you to construct a SPARQL query.\n",
    "Do not include any text except the SPARQL query generated.\n",
    "\n",
    "Useful individuals for classes and properties that can help formulating the query:\n",
    "{individuals}\n",
    "\n",
    "The question is:\n",
    "{prompt}\n",
    "\"\"\"\n",
    "SPARQL_GENERATION_SELECT_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"schema\", \"individuals\", \"prompt\"], \n",
    "    template=SPARQL_SELECT_TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPHDB_SPARQL_NO_RESULT_TEMPLATE = \"\"\"\n",
    "This following SPARQL query delimited by triple backticks\n",
    "```\n",
    "{generated_sparql}\n",
    "```\n",
    "is valid, but it didn't return any results from the graph.\n",
    "Please try a slightly different SPARQL query.\n",
    "This is the question in natural language that should be answered by the query, delimited by triple backticks:\n",
    "```\n",
    "{prompt}\n",
    "```\n",
    "Do not include any explanations or apologies in your responses.\n",
    "Do not wrap the query in backticks.\n",
    "Do not include any text except the SPARQL query generated.\n",
    "The ontology schema delimited by triple backticks in Owl format is:\n",
    "```\n",
    "{schema}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "GRAPHDB_SPARQL_NO_RESULT_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"generated_sparql\", \"prompt\", \"schema\"],\n",
    "    template=GRAPHDB_SPARQL_NO_RESULT_TEMPLATE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard prompts\n",
    "\n",
    "sparql_generation_prompt: BasePromptTemplate = SPARQL_GENERATION_SELECT_PROMPT\n",
    "\n",
    "sparql_fix_prompt: BasePromptTemplate = GRAPHDB_SPARQL_FIX_PROMPT\n",
    "\n",
    "qa_prompt: BasePromptTemplate = GRAPHDB_QA_PROMPT\n",
    "\n",
    "no_result_prompt: BasePromptTemplate = GRAPHDB_SPARQL_NO_RESULT_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparql_generation_chain = sparql_generation_prompt | llm\n",
    "\n",
    "sparql_fix_chain = sparql_fix_prompt | llm\n",
    "\n",
    "qa_chain = qa_prompt | llm\n",
    "\n",
    "no_result_chain = no_result_prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "_run_manager = CallbackManagerForChainRun.get_noop_manager()\n",
    "callbacks = _run_manager.get_child()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPARQLQueryHandler:\n",
    "    def __init__(self, max_sparql_retries: int = 3):\n",
    "        self.max_sparql_retries = max_sparql_retries\n",
    "\n",
    "    def log_prepared_sparql_query(self, _run_manager, generated_query: str) -> None:\n",
    "        _run_manager.on_text(\"Generated SPARQL:\", end=\"\\n\", verbose=True)\n",
    "        _run_manager.on_text(generated_query, color=\"green\", end=\"\\n\", verbose=True)\n",
    "\n",
    "    def log_invalid_sparql_query(self, _run_manager, generated_query: str, error_message: str) -> None:\n",
    "        _run_manager.on_text(\"Invalid SPARQL query: \", end=\"\\n\", verbose=True)\n",
    "        _run_manager.on_text(generated_query, color=\"red\", end=\"\\n\", verbose=True)\n",
    "        _run_manager.on_text(\"SPARQL Query Parse Error: \", end=\"\\n\", verbose=True)\n",
    "        _run_manager.on_text(error_message, color=\"red\", end=\"\\n\\n\", verbose=True)\n",
    "\n",
    "    def prepare_sparql_query(self, _run_manager, generated_sparql: str) -> str:\n",
    "        prepareQuery(generated_sparql)\n",
    "        self.log_prepared_sparql_query(_run_manager, generated_sparql)\n",
    "        return generated_sparql\n",
    "\n",
    "    def get_prepared_sparql_query(self, _run_manager, generated_sparql: str, ontology_schema: str) -> str:\n",
    "        try:\n",
    "            return self.prepare_sparql_query(_run_manager, generated_sparql)\n",
    "        except Exception as e:\n",
    "            retries = 0\n",
    "            error_message = str(e)\n",
    "            self.log_invalid_sparql_query(_run_manager, generated_sparql, error_message)\n",
    "            print(\"Error message: \", error_message)\n",
    "\n",
    "            while retries < self.max_sparql_retries:\n",
    "                try:\n",
    "                    sparql_fix_chain_result = sparql_fix_chain.invoke(\n",
    "                        {\n",
    "                            \"error_message\": error_message,\n",
    "                            \"generated_sparql\": generated_sparql,\n",
    "                            \"schema\": ontology_schema,\n",
    "                        }\n",
    "                    )\n",
    "                    generated_sparql = sparql_fix_chain_result.content\n",
    "                    return self.prepare_sparql_query(_run_manager, generated_sparql)\n",
    "                except Exception as e:\n",
    "                    retries += 1\n",
    "                    parse_exception = str(e)\n",
    "                    print(\"Error message (parse_exception): \", parse_exception)\n",
    "                    self.log_invalid_sparql_query(_run_manager, generated_sparql, parse_exception)\n",
    "\n",
    "            print(\"The generated SPARQL query is invalid.\")\n",
    "            return None\n",
    "\n",
    "    def execute_query(self, g, query: str) -> List[rdflib.query.ResultRow]:\n",
    "        try:\n",
    "            rdf_results = g.query(query)\n",
    "\n",
    "            results_list = []\n",
    "            for row in rdf_results:\n",
    "                results_list.append(row)\n",
    "\n",
    "            return results_list\n",
    "        except Exception:\n",
    "            print(\"Failed to execute the generated SPARQL query.\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStoreHandler:\n",
    "    def __init__(self, llm, vector_store, ontology_vector_store, k: int = 5):\n",
    "        self.llm = llm\n",
    "        self.vector_store = vector_store\n",
    "        self.ontology_vector_store = ontology_vector_store\n",
    "        self.k = k\n",
    "    \n",
    "    def perform_search(self, system_prompt: str, input_query: str):\n",
    "        raw_query = str(system_prompt + input_query)    \n",
    "        search_query = self.llm.invoke(raw_query)\n",
    "        \n",
    "        # Documents Vector Store        \n",
    "        index_results = self.vector_store.hybrid_search(\n",
    "            query=search_query.content, k=self.k\n",
    "        )\n",
    "        \n",
    "        # Ontology Vector Store\n",
    "        ontology_index_results = self.ontology_vector_store.hybrid_search(\n",
    "            query=search_query.content, k=self.k\n",
    "        )\n",
    "        \n",
    "        return index_results, ontology_index_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_query = \"What are the common recommendations between CLO and DPO?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate classes\n",
    "sparql_handler = SPARQLQueryHandler(max_sparql_retries=3)\n",
    "vectorstore_handler = VectorStoreHandler(llm, vector_store, ontology_vector_store, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_individuals = \"\"\"\n",
    "cro:CLO_Operation rdf:type owl:NamedIndividual ,\n",
    "                           cro:Operation ;\n",
    "                  cro:hasAddress \"PO Box 21, 6720 Wickham WA, AU\" ;\n",
    "                  cro:hasCRAType \"MFL\" ;\n",
    "                  cro:hasDateOfSurvey \"2023-08-29T00:00:00\" ,\n",
    "                                      \"2023-09-01T00:00:00\" ;\n",
    "                  cro:hasLatitude -20.6011 ;\n",
    "                  cro:hasLongitude 117.1701 ;\n",
    "                  cro:hasName \"Cape Lambert Operations\" ;\n",
    "                  cro:hasPhone \"+61 (0) 8 9183 7004\" .\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sparql = \"\"\"PREFIX cro: <http://WSP.org/ontology/cro#>\n",
    "SELECT ?recommendation\n",
    "WHERE {\n",
    "    ?recommendation cro:improvesOperation ?operation1, ?operation2 .\n",
    "    ?operation1 cro:hasName \"Cape Lambert Operations\" .\n",
    "    ?operation2 cro:hasName \"Dampier Port Operations\" .\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'useful_individuals' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# SPARQL Query Generation\u001b[39;00m\n\u001b[0;32m      2\u001b[0m sparql_generation_chain_result \u001b[38;5;241m=\u001b[39m sparql_generation_chain\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m----> 3\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_query, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindividuals\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43museful_individuals\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mschema\u001b[39m\u001b[38;5;124m\"\u001b[39m: ontology}\n\u001b[0;32m      4\u001b[0m )\n\u001b[0;32m      6\u001b[0m raw_sparql \u001b[38;5;241m=\u001b[39m sparql_generation_chain_result\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(raw_sparql)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'useful_individuals' is not defined"
     ]
    }
   ],
   "source": [
    "# SPARQL Query Generation\n",
    "sparql_generation_chain_result = sparql_generation_chain.invoke(\n",
    "    {\"prompt\": input_query, \"individuals\": useful_individuals, \"schema\": ontology}\n",
    ")\n",
    "\n",
    "raw_sparql = sparql_generation_chain_result.content\n",
    "\n",
    "print(raw_sparql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREFIX cro: <http://WSP.org/ontology/cro#>\n",
      "SELECT ?recommendation\n",
      "WHERE {\n",
      "    ?recommendation cro:improvesOperation ?operation1, ?operation2 .\n",
      "    ?operation1 cro:hasName \"Cape Lambert Operations\" .\n",
      "    ?operation2 cro:hasName \"Dampier Port Operations\" .\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generated_sparql = sparql_handler.get_prepared_sparql_query(_run_manager, raw_sparql, ontology)\n",
    "\n",
    "print(generated_sparql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparql_results = sparql_handler.execute_query(g, generated_sparql)\n",
    "\n",
    "sparql_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a second iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If no results, iterate up to a max number of attempts\n",
    "query_retries = 0\n",
    "while sparql_results == [] and query_retries < max_query_retries:\n",
    "    print(f\"## No results retrieved by the query, generating a new query. Attempt {query_retries + 1}\")\n",
    "    query_retries += 1\n",
    "\n",
    "    # Use no_result_chain to generate a new query\n",
    "    no_result_chain_result = no_result_chain.invoke(\n",
    "        {\n",
    "            \"generated_sparql\": generated_sparql,\n",
    "            \"prompt\": input_query,\n",
    "            \"schema\": ontology,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Get the newly generated SPARQL query and execute it\n",
    "    generated_sparql = no_result_chain_result.content\n",
    "    generated_sparql = sparql_handler.get_prepared_sparql_query(_run_manager, generated_sparql, ontology)\n",
    "    sparql_results = sparql_handler.execute_query(generated_sparql)\n",
    "\n",
    "# Handle case when no results were retrieved after max iterations\n",
    "if sparql_results == []:\n",
    "    print(f\"No results after {max_query_retries} attempts for query: {input_query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer generation\n",
    "qa_chain_result = qa_chain.invoke(\n",
    "    {\"prompt\": input_query, \"context\": sparql_results}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain_result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the critical risk scenarios:\n",
      "\n",
      "1. **DPO-01**: A 1 in 100-year Category 5 tropical cyclone directly impacts the Karratha and Dampier townships and the Dampier port operations, causing extensive damage in the region, storm surge, and flooding. Local buildings, houses, and infrastructure, including hospitals, schools, electricity, and water supplies, suffer a range of damage. Non-essential personnel without safe housing are evacuated. Older steel-clad buildings at Dampier port operations lose some cladding. Torrential rain causes flooding in rail dumper vaults and some conveyor tunnels, and water ingress occurs into substations. Damage occurs to the causeway and approach jetty decking at EII. The shipping channel requires surveying and some subsequent dredging. Loss of some navigation aids occurs, but no major complete loss of Dampier port operations structures or facilities happens.\n",
      "\n",
      "2. **DPO-04**: One of the two Parker Point shiploaders is lost due to extreme weather conditions, structural failure, exceeding design loads, high impact collision, or loss of stability caused by a vessel rising on an incoming tide. Other perils such as fire are not likely to result in complete loss. Similar loss scenarios are possible for the EII shiploader but with less consequence.\n",
      "\n",
      "3. **DPO-05**: The dredged portion of the common departure channel for both EII and PPt becomes blocked due to a grounded ship, impacting loaded vessels departing from both PPt and EII. Causes could include collision, on-board fire, or grounding. The sunken ship prevents passage of fully loaded bulk carriers and may cause hydrocarbon release. Port operations are closed for two weeks for investigations, clean-up, and procedural approvals.\n",
      "\n",
      "4. **DPO-07**: A vessel suffers damage and sinks at EII berth due to overloading, fire, or collision. A similar scenario is possible at PPt, but the consequence is less severe. Both shiploaders at PPt can access two berth pockets, minimizing business interruption.\n",
      "\n",
      "5. **DPO-10**: An EII reclaimer (bridge type) topples over due to high speed or overdriving onto the transfer car, caused by an interlock failure and/or operator error. This results in damage to the Transfer Car and irreparable damage to the machine structure.\n",
      "\n",
      "6. **DPO-11**: One of the two PPt screenhouses suffers significant damage in a fire started by faulty equipment or inadequately controlled hot work. The fire spreads, causing extensive damage to feeders, screens, and some internal structural steel. The main columns, foundations, and bins are undamaged or recoverable. The fire causes complete interruption to one PPt outloading circuit and shiploader. A similar fire could affect the EII screen house with a lower loss estimate for Business Interruption.\n",
      "\n",
      "7. **DPO-12**: A substation is damaged due to a circuit breaker fault leading to arcing and a fire. The fire spreads to cables above the cabinets, destroying all internal equipment. All switchboards and control panels require replacement, and all cables need re-termination.\n",
      "\n",
      "8. **DPO-16**: A fire starts in the conveyor transfer structure at the Jetty/Wharf transfer point, destroying all mechanical components and buckling the steel transfer structure. The adjoining substation is damaged and requires repairs.\n",
      "\n",
      "9. **DPO-20**: A large vessel loses control near the EII wharf and impacts the wharf, causing damage to part of the wharf structure and two dolphins. A similar scenario at PPt would likely cause less business interruption.\n",
      "\n",
      "10. **DPO-22**: A major failure occurs in one of the PPt or EII dual cell car dumpers, possibly due to structural/mechanical damage from derailment, impact, maloperation, or defective components. Inadvertent dumping of non-ore rolling stock or dropped ore cars involves complex extrication. Other failures like fire in the hydraulic room or extraction fan mechanical failure have shorter recovery times.\n",
      "\n",
      "11. **DPO-23**: A fire occurs on the fuel carrier during unloading, not controlled by on-board fire suppression. RTIO tugs and neighboring facility tugs attempt to suppress the fire. The eastern end of the wharf and nearby conveyors are significantly damaged. The shiploaders avoid damage by being parked at a safe distance.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_xlsx = os.path.join(\"..\", \"Generated Questions.xlsx\")\n",
    "\n",
    "questions_df = pd.read_excel(questions_xlsx)\n",
    "questions = questions_df[\"Question\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(columns=[\"input_query\", \"generated_sparql\", \"query_results\", \"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "sparql_handler = SPARQLQueryHandler(max_sparql_retries=3)\n",
    "vectorstore_handler = VectorStoreHandler(llm, vector_store, ontology_vector_store, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_query in questions[0:5]:\n",
    "    # SPARQL Query Generation\n",
    "    sparql_generation_chain_result = sparql_generation_chain.invoke(\n",
    "        {\"prompt\": input_query, \"schema\": ontology}\n",
    "    )\n",
    "\n",
    "    raw_sparql = sparql_generation_chain_result.content\n",
    "    generated_sparql = sparql_handler.get_prepared_sparql_query(_run_manager, raw_sparql, ontology)\n",
    "    sparql_results = sparql_handler.execute_query(generated_sparql)\n",
    "\n",
    "    # If no results, iterate up to a max number of attempts\n",
    "    query_retries = 0\n",
    "    while sparql_results == [] and query_retries < max_query_retries:\n",
    "        print(f\"## No results retrieved by the query, generating a new query. Attempt {query_retries + 1}\")\n",
    "        query_retries += 1\n",
    "\n",
    "        # Use no_result_chain to generate a new query\n",
    "        no_result_chain_result = no_result_chain.invoke(\n",
    "            {\n",
    "                \"generated_sparql\": generated_sparql,\n",
    "                \"prompt\": input_query,\n",
    "                \"schema\": ontology,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Get the newly generated SPARQL query and execute it\n",
    "        generated_sparql = no_result_chain_result.content\n",
    "        generated_sparql = sparql_handler.get_prepared_sparql_query(_run_manager, generated_sparql, ontology)\n",
    "        sparql_results = sparql_handler.execute_query(generated_sparql)\n",
    "\n",
    "    # Handle case when no results were retrieved after max iterations\n",
    "    if sparql_results == []:\n",
    "        print(f\"No results after {max_query_retries} attempts for query: {input_query}\")\n",
    "\n",
    "    # Answer generation\n",
    "    qa_chain_result = qa_chain.invoke(\n",
    "        {\"prompt\": input_query, \"context\": sparql_results}\n",
    "    )\n",
    "\n",
    "    result = qa_chain_result.content\n",
    "    print(result)\n",
    "\n",
    "    # Store results\n",
    "    test_result = pd.DataFrame([{\n",
    "        \"input_query\": str(input_query),\n",
    "        \"generated_sparql\": str(generated_sparql),\n",
    "        \"query_results\": str(sparql_results),\n",
    "        \"result\": str(result),\n",
    "    }])\n",
    "\n",
    "    results_df = pd.concat([results_df, test_result], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate question codes\n",
    "code_sequence = [f\"CRD-{str(i).zfill(2)}\" for i in range(1, len(results_df) + 1)]\n",
    "try:\n",
    "    results_df.insert(0, 'code', code_sequence)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'Outcome' and 'Validation' columns\n",
    "results_df['outcome'] = results_df['query_results'].apply(lambda x: 0 if x == \"[]\" else 1)\n",
    "positive_outcome_count = results_df['outcome'].sum()\n",
    "total_outcome_count = len(results_df)\n",
    "results_df['validation'] = ((positive_outcome_count / total_outcome_count) * 100).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics\n",
    "validation_percentage = (positive_outcome_count / total_outcome_count) * 100\n",
    "failed_questions_df = results_df[results_df['outcome'] == 0]\n",
    "failed_question_count = len(failed_questions_df)\n",
    "failed_question_codes = failed_questions_df['code'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data to Excel\n",
    "timestamp = pd.Timestamp.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "with pd.ExcelWriter(f\"validation/Sparql Validation_{timestamp}.xlsx\", engine='xlsxwriter') as writer:\n",
    "\n",
    "    # Save results data to Sheet1\n",
    "    results_df.to_excel(writer, sheet_name='Sheet1', index=False)\n",
    "\n",
    "    # Create summary data for Sheet2\n",
    "    summary_data = {\n",
    "        'Statistic': ['Accuracy', 'Error Count', 'Error Codes'],\n",
    "        'Value': [f'{validation_percentage:.2f}%', failed_question_count, ', '.join(failed_question_codes)]\n",
    "    }\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "    summary_df.to_excel(writer, sheet_name='Sheet2', index=False)\n",
    "\n",
    "    # Access the workbook and worksheet for formatting\n",
    "    workbook = writer.book\n",
    "    worksheet1 = writer.sheets['Sheet1']\n",
    "    worksheet2 = writer.sheets['Sheet2']\n",
    "\n",
    "    # Set column width\n",
    "    worksheet1.set_column(0, len(results_df.columns) - 1, 30)\n",
    "    worksheet2.set_column(0, 1, 20)\n",
    "\n",
    "    # Add conditional formatting to the outcome column in Sheet1\n",
    "    outcome_range = f'F2:F{len(results_df) + 1}'\n",
    "    format_pass = workbook.add_format({'bg_color': '#77DD77'})  # Green for pass (1)\n",
    "    format_fail = workbook.add_format({'bg_color': '#FF0000'})  # Red for fail (0)\n",
    "\n",
    "    worksheet1.conditional_format(outcome_range, {\n",
    "        'type': 'cell',\n",
    "        'criteria': '==',\n",
    "        'value': 1,\n",
    "        'format': format_pass\n",
    "    })\n",
    "\n",
    "    worksheet1.conditional_format(outcome_range, {\n",
    "        'type': 'cell',\n",
    "        'criteria': '==',\n",
    "        'value': 0,\n",
    "        'format': format_fail\n",
    "    })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
