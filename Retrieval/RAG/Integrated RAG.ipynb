{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from __future__ import annotations\n",
    "from sparql_prompts import *    \n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_openai import AzureOpenAIEmbeddings    \n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_core.callbacks.manager import CallbackManager, CallbackManagerForChainRun\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain_core.prompts.base import BasePromptTemplate\n",
    "from pydantic import Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain_community.chains.graph_qa.prompts import (\n",
    "    GRAPHDB_QA_PROMPT,\n",
    "    GRAPHDB_SPARQL_FIX_PROMPT,\n",
    "    GRAPHDB_SPARQL_GENERATION_PROMPT,\n",
    ")\n",
    "from langchain_community.graphs import OntotextGraphDBGraph\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from langsmith import Client\n",
    "from langsmith import traceable\n",
    "\n",
    "from langchain_community.graphs import OntotextGraphDBGraph\n",
    "from typing import TYPE_CHECKING, Any, Dict, List, Optional\n",
    "if TYPE_CHECKING:\n",
    "    import rdflib\n",
    "    \n",
    "from rdflib import Graph    \n",
    "from rdflib.plugins.sparql import prepareQuery\n",
    "from rdflib import Namespace\n",
    "\n",
    "import textwrap\n",
    "import pandas as pd\n",
    "import requests\n",
    "from typing import List, OrderedDict\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(os.path.join(\"..\", \"Azure OpenAI credentials.env\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_endpoint = os.environ['GLOBAL_AZURE_ENDPOINT']\n",
    "openai_api_key = os.environ['GLOBAL_OPENAI_API_KEY']\n",
    "\n",
    "openai_deployment_name = os.environ['GLOBAL_GPT_DEPLOYMENT_NAME']\n",
    "openai_api_version = os.environ['GLOBAL_OPENAI_API_VERSION']\n",
    "embedding_model = os.environ['GLOBAL_EMBEDDING_MODEL']\n",
    "embedding_deployment_name = os.environ['GLOBAL_EMBEDDING_DEPLOYMENT_NAME']\n",
    "\n",
    "search_endpoint = os.environ['SEARCH_ENDPOINT']\n",
    "search_api_key = os.environ['SEARCH_API_KEY']\n",
    "search_api_version = os.environ['SEARCH_API_VERSION']\n",
    "search_service_name = os.environ['SEARCH_SERVICE_NAME']\n",
    "search_sem_config = os.environ['SEARCH_SEMANTIC_CONFIG_NAME']\n",
    "\n",
    "# langsmith_api_key = os.environ['LANGSMITH_API_KEY']\n",
    "\n",
    "search_url = f\"https://{search_service_name}.search.windows.net/\"\n",
    "search_credential = AzureKeyCredential(search_api_key)\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=openai_deployment_name, \n",
    "    openai_api_version=openai_api_version, \n",
    "    openai_api_key=openai_api_key, \n",
    "    azure_endpoint=azure_endpoint, \n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=embedding_deployment_name,\n",
    "    api_version=openai_api_version,\n",
    "    api_key=openai_api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    ")\n",
    "\n",
    "doc_index_name: str = \"crd-vector-store\"\n",
    "ontology_index_name: str = \"crd-ontologies-desc\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph database config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_inst_ontology_path = os.path.join(\"..\", \"ontology/non_inst\", \"CRA V16 MFL.ttl\")\n",
    "\n",
    "with open(non_inst_ontology_path, 'r', encoding='utf-8') as file:\n",
    "        non_inst_ontology = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N320a106ba5b64e24a3285b62b92cab7a (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inst_ontology_path = os.path.join(\"..\", \"ontology/inst\", \"CRA V16.2 MFL Instantiated Ontology.ttl\")\n",
    "\n",
    "g = Graph()\n",
    "g.parse(inst_ontology_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sparql_retries = 5  # Max number of attempts at generating a correct SPARQL query\n",
    "max_query_retries = 3   # Max number of attempts at generating a SPARQL query that returns at least one result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph has 9253 triples.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Graph has {len(g)} triples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Oyu Tolgoi Copper Mine\n",
      "Scenario: http://WSP.org/ontology/cro#OTC_critical_risk_assesment_report_2023\n"
     ]
    }
   ],
   "source": [
    "# Define a general SPARQL query to retrieve any subject, predicate, and object\n",
    "general_query = \"\"\"\n",
    "PREFIX cro: <http://WSP.org/ontology/cro#>\n",
    "SELECT ?operationName ?criticalRiskScenario\n",
    "WHERE {\n",
    "    VALUES ?operationName {\"Oyu Tolgoi Copper Mine\"} .\n",
    "    ?operation cro:hasName ?operationName ;\n",
    "               cro:hasRiskAsessmentReport ?riskAssessmentReport .\n",
    "    ?riskAssessmentReport cro:hasIdentifiedCriticalRiskScenarios ?criticalRiskScenario .\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Run the query\n",
    "results = g.query(general_query)\n",
    "\n",
    "# Iterate over the results and print\n",
    "for row in results:\n",
    "    print(f\"Name: {row.operationName}\", f\"Scenario: {row.criticalRiskScenario}\", sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9253"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard prompts\n",
    "\n",
    "sparql_generation_prompt: BasePromptTemplate = SPARQL_GENERATION_SELECT_PROMPT\n",
    "\n",
    "sparql_fix_prompt: BasePromptTemplate = GRAPHDB_SPARQL_FIX_PROMPT\n",
    "\n",
    "qa_prompt: BasePromptTemplate = GRAPHDB_QA_PROMPT\n",
    "\n",
    "no_result_prompt: BasePromptTemplate = SPARQL_NO_RESULT_PROMPT\n",
    "\n",
    "combined_prompt: BasePromptTemplate = COMBINED_QA_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparql_generation_chain = sparql_generation_prompt | llm\n",
    "\n",
    "sparql_fix_chain = sparql_fix_prompt | llm\n",
    "\n",
    "qa_chain = qa_prompt | llm\n",
    "\n",
    "no_result_chain = no_result_prompt | llm\n",
    "\n",
    "combined_chain = combined_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparql_templates_path = os.path.join(\"..\", \"ontology/sparql_templates\", \"sparql templates.txt\")\n",
    "\n",
    "with open(sparql_templates_path, 'r', encoding='utf-8') as file:\n",
    "        sparql_templates = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Template #1: Retrieving Sub-Elements of a Specific Operation\\n\\nTo find the sub-elements of a specific operation named \"XXX,\" you can use the following SPARQL query. Replace \"XXX\" with the desired operation name:\\nPREFIX cro: <http://WSP.org/ontology/cro#>\\nSELECT ?riskManagementSubElement \\nWHERE {\\n    ?operation cro:hasName \"XXX\" ;\\n               cro:hasRiskAsessmentReport ?riskAssessmentReport .\\n    ?riskAssessmentReport cro:hasRiskManagementSystem ?riskManagementSystem .\\n    ?riskManagementSystem cro:hasRiskManagementSubElement ?riskManagementSubElement .\\n}\\n\\n\\n\\nTemplate #2: Retrieving Risk Management System of a Specific Operation\\n\\nTo look at the risk management system associated with a specific operation named \"XXX,\" you can use the following SPARQL query. Replace \"XXX\" with the operation name you\\'re interested in:\\nPREFIX cro: <http://WSP.org/ontology/cro#>\\nSELECT ?riskManagementSystem\\nWHERE {\\n   ?operation cro:hasName \"XXX\" ;\\n               cro:hasRiskAsessmentReport ?riskAssessmentReport .\\n    ?riskAssessmentReport cro:hasRiskManagementSystem ?riskManagementSystem .\\n}\\n\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparql_templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "_run_manager = CallbackManagerForChainRun.get_noop_manager()\n",
    "callbacks = _run_manager.get_child()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPARQLQueryHandler:\n",
    "    def __init__(self, max_sparql_retries: int = 3):\n",
    "        self.max_sparql_retries = max_sparql_retries\n",
    "\n",
    "    def log_prepared_sparql_query(self, _run_manager, generated_query: str) -> None:\n",
    "        _run_manager.on_text(\"Generated SPARQL:\", end=\"\\n\", verbose=True)\n",
    "        _run_manager.on_text(generated_query, color=\"green\", end=\"\\n\", verbose=True)\n",
    "\n",
    "    def log_invalid_sparql_query(self, _run_manager, generated_query: str, error_message: str) -> None:\n",
    "        _run_manager.on_text(\"Invalid SPARQL query: \", end=\"\\n\", verbose=True)\n",
    "        _run_manager.on_text(generated_query, color=\"red\", end=\"\\n\", verbose=True)\n",
    "        _run_manager.on_text(\"SPARQL Query Parse Error: \", end=\"\\n\", verbose=True)\n",
    "        _run_manager.on_text(error_message, color=\"red\", end=\"\\n\\n\", verbose=True)\n",
    "\n",
    "    def prepare_sparql_query(self, _run_manager, generated_sparql: str) -> str:\n",
    "        prepareQuery(generated_sparql)\n",
    "        self.log_prepared_sparql_query(_run_manager, generated_sparql)\n",
    "        return generated_sparql\n",
    "\n",
    "    def get_prepared_sparql_query(self, _run_manager, generated_sparql: str, ontology_schema: str) -> str:\n",
    "        try:\n",
    "            return self.prepare_sparql_query(_run_manager, generated_sparql)\n",
    "        except Exception as e:\n",
    "            retries = 0\n",
    "            error_message = str(e)\n",
    "            self.log_invalid_sparql_query(_run_manager, generated_sparql, error_message)\n",
    "            print(\"Error message: \", error_message)\n",
    "\n",
    "            while retries < self.max_sparql_retries:\n",
    "                try:\n",
    "                    sparql_fix_chain_result = sparql_fix_chain.invoke(\n",
    "                        {\n",
    "                            \"error_message\": error_message,\n",
    "                            \"generated_sparql\": generated_sparql,\n",
    "                            \"schema\": ontology_schema,\n",
    "                        }\n",
    "                    )\n",
    "                    generated_sparql = sparql_fix_chain_result.content\n",
    "                    return self.prepare_sparql_query(_run_manager, generated_sparql)\n",
    "                except Exception as e:\n",
    "                    retries += 1\n",
    "                    parse_exception = str(e)\n",
    "                    print(\"Error message (parse_exception): \", parse_exception)\n",
    "                    self.log_invalid_sparql_query(_run_manager, generated_sparql, parse_exception)\n",
    "\n",
    "            print(\"The generated SPARQL query is invalid.\")\n",
    "            return None\n",
    "\n",
    "    def execute_query(self, g, query: str) -> List[rdflib.query.ResultRow]:\n",
    "        try:\n",
    "            rdf_results = g.query(query)\n",
    "\n",
    "            results_list = []\n",
    "            for row in rdf_results:\n",
    "                results_list.append(row)\n",
    "\n",
    "            return results_list\n",
    "        except Exception:\n",
    "            print(\"Failed to execute the generated SPARQL query.\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStoreHandler:\n",
    "    def __init__(self, llm, doc_index_name, ontology_index_name, k: int = 5):\n",
    "        self.llm = llm\n",
    "        self.k = k\n",
    "        self.doc_index_name = doc_index_name\n",
    "        self.ontology_index_name = ontology_index_name\n",
    "        self.headers = {'Content-Type': 'application/json',\n",
    "            'api-key': search_api_key}\n",
    "        self.params = {'api-version': search_api_version}\n",
    "    \n",
    "    def perform_vector_search(self, system_prompt: str, input_query: str):\n",
    "        raw_query = str(system_prompt + input_query)    \n",
    "        search_query = self.llm.invoke(raw_query)\n",
    "        \n",
    "        search_query = str(search_query.content)\n",
    "        print(\"### Querying pdf document database: \", search_query)\n",
    "                    \n",
    "        # Documents Vector Store        \n",
    "        search_payload = {\n",
    "            \"search\": search_query,\n",
    "            \"queryType\": \"semantic\",\n",
    "            \"vectorQueries\": [{\"text\": search_query, \"fields\": \"embedding\", \"kind\": \"text\"}],\n",
    "            \"semanticConfiguration\": search_sem_config,\n",
    "            \"captions\": \"extractive\",\n",
    "            \"answers\": \"extractive|count-3\",\n",
    "            \"queryLanguage\": \"en-us\",\n",
    "            \"count\": \"true\",\n",
    "            \"top\": self.k\n",
    "        }\n",
    "        \n",
    "        resp = requests.post(f\"{search_endpoint}/indexes/{self.doc_index_name}/docs/search\",\n",
    "                            data=json.dumps(search_payload), headers=self.headers, params=self.params)\n",
    "        search_results = resp.json()\n",
    "\n",
    "        content = dict()\n",
    "        ordered_content = OrderedDict()\n",
    "\n",
    "        for result in search_results['value']:\n",
    "            if result['@search.rerankerScore'] > 1: #Change treshold if needed\n",
    "                content[result['id']] = {\n",
    "                    \"doc_path\": result['doc_path'],\n",
    "                    \"chunk\": result['chunk'],\n",
    "                    \"score\": result['@search.rerankerScore']\n",
    "                }\n",
    "\n",
    "        topk = self.k\n",
    "        \n",
    "        count = 0  # To keep track of the number of results added\n",
    "        for id in sorted(content, key=lambda x: content[x][\"score\"], reverse=True):\n",
    "            ordered_content[id] = content[id]\n",
    "            count += 1\n",
    "            if count >= topk:  # Stop after adding topK results\n",
    "                break\n",
    "            \n",
    "        index_results = []\n",
    "        for key, value in ordered_content.items():\n",
    "            try:\n",
    "                index_results.append(Document(page_content=value[\"chunk\"], metadata={\n",
    "                    \"doc_path\": value['doc_path'], \"score\": value[\"score\"]}))\n",
    "\n",
    "            except:\n",
    "                print(\"An exception occurred\")\n",
    "    \n",
    "        return index_results\n",
    "        \n",
    "    def perform_graph_search(self, system_prompt: str, input_query: str):\n",
    "        \n",
    "        raw_query = str(system_prompt + input_query)    \n",
    "        search_query = self.llm.invoke(raw_query)\n",
    "        \n",
    "        search_query = str(search_query.content)\n",
    "        print(\"### Querying ontology database: \", search_query)\n",
    "        \n",
    "        # Ontology Vector Store\n",
    "\n",
    "        search_payload = {\n",
    "            \"search\": search_query,\n",
    "            \"queryType\": \"semantic\",\n",
    "            \"vectorQueries\": [{\"text\": search_query, \"fields\": \"embedding\", \"kind\": \"text\"}],\n",
    "            \"semanticConfiguration\": search_sem_config,\n",
    "            \"captions\": \"extractive\",\n",
    "            \"answers\": \"extractive|count-3\",\n",
    "            \"queryLanguage\": \"en-us\",\n",
    "            \"count\": \"true\",\n",
    "            \"filter\": \"doc_path eq 'CRA V16.2 MFL Instantiated Ontology.ttl'\",\n",
    "            \"filter\": \"doc_path eq 'CRA V17 MFL & NLE Instantiated Ontology.ttl'\",\n",
    "            \"top\": self.k\n",
    "        }\n",
    "        \n",
    "        resp = requests.post(f\"{search_endpoint}/indexes/{self.ontology_index_name}/docs/search\",\n",
    "                            data=json.dumps(search_payload), headers=self.headers, params=self.params)\n",
    "        search_results = resp.json()\n",
    "\n",
    "        content = dict()\n",
    "        ordered_content = OrderedDict()\n",
    "\n",
    "        for result in search_results['value']:\n",
    "            if result['@search.rerankerScore'] > 1: #Change treshold if needed\n",
    "                content[result['id']] = {\n",
    "                    \"doc_path\": result['doc_path'],\n",
    "                    \"individual\": result['individual'],\n",
    "                    \"score\": result['@search.rerankerScore']\n",
    "                }\n",
    "\n",
    "        topk = self.k\n",
    "        \n",
    "        count = 0  # To keep track of the number of results added\n",
    "        for id in sorted(content, key=lambda x: content[x][\"score\"], reverse=True):\n",
    "            ordered_content[id] = content[id]\n",
    "            count += 1\n",
    "            if count >= topk:  # Stop after adding topK results\n",
    "                break\n",
    "            \n",
    "        index_results = []\n",
    "        for key, value in ordered_content.items():\n",
    "            try:\n",
    "                index_results.append(Document(page_content=value[\"individual\"], metadata={\n",
    "                    \"doc_path\": value['doc_path'], \"score\": value[\"score\"]}))\n",
    "\n",
    "            except:\n",
    "                print(\"An exception occurred\")\n",
    "    \n",
    "        return index_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define questions manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"List all the Critical Risk Scenarios that impact Cape Lambert\", # Q0\n",
    "    \"Provide me with a brief description of Dampier Port\", # Q1\n",
    "    \"Describe the 2021 Dredging Campaign carried out at Dampier Port\", # Q2\n",
    "    \"Compare the critical risk scenarios of CLO and DPO\", # Q3\n",
    "    \"What are the common recommendations between CLO and DPO?\", # Q4\n",
    "    \"What equipments are being impacted by the risk with the highest total loss at Dampier Port operation\", # Q5\n",
    "    \"What facilities are being impacted by risks at Oyu Tolgoi and  Dampier Port operations\", # Q6\n",
    "    \"What is the total business interuption of top 5 risks in Oyu Tolgoi operation\", # Q7\n",
    "    \"What are the key information about the critical assessment report  conducted for Dampier Port operation\", # Q8\n",
    "    \"what  are the electricity sources of Oyu Tolgoi and  Dampier Port operations\", # Q9\n",
    "    \"what  are the common sources of electricity  between Oyu Tolgoi and  Dampier Port operations\", # Q10\n",
    "    \"What scenarios exist for cascading risks happening at  Dampier Port operation\", # Q11\n",
    "    \"List all equipments with their corresponding critical risk scenrairo at Dampier Port Operation\", # Q12\n",
    "    \"List all facilities with their corresponding critical risk scenrairo at Dampier Port Operation\", # Q13\n",
    "    \"There is a predicted high temprature at Dampier port operation, list all potentianl critical risk scenarios and recommendations associated along their IDs. Explain how these scenarios are linked to the high temperature\", # Q14\n",
    "    \"List all the existing maintenance projects at Dampier port operation\", # Q15\n",
    "    \"is there any common recommendations for critical risk scenarios accross the operations you have in your knowledge base\", # Q16\n",
    "    \"Give me a comparisson of the critical risk scenarios in Dampier Port Operation and Cape Lambert Operation. What similarities can we observe on both operations?\", # Q17\n",
    "    \"Out of DPO and CLO, which operation is more likely to be hit by a cyclone\", # Q18\n",
    "    \"What are the top 5 risks in terms of total loss across the sites for Dampier Port, Cape Lambert and Oyu Tolgoi\", # Q19\n",
    "    \"What is the recommendation that mitigates the most in terms of total loss across the sites for Dampier Port, Cape Lambert and Oyu Tolgoi. \", # Q20\n",
    "    \"What are the most likely risks across the sites for Dampier Port, Cape Lambert and Oyu Tolgoi. Include the likelihood ranking\", # Q21\n",
    "    \"Give me a list of all the risks with likelihood of 'Possible' across the sites for Dampier Port, Cape Lambert and Oyu Tolgoi. \", # Q22\n",
    "    \"What year was the risk Scenario CLO-03 identified?\", # Q23\n",
    "    \"What are the main sources of energy for the power station providing power to Dampier Port\", # Q24\n",
    "    \"What are the pieces of equipement that are found in large critical risk scenarios. \", # Q25\n",
    "    \"What are the most common types of maintenance being conducted at across the sites in your knowledge base\", # Q26\n",
    "    \"What are the main differences between Dampier Port and Cape Lambert\", # Q27\n",
    "    \"Is there a single point of failure in Cape Lambert in terms of electricity supply?\", # Q28\n",
    "    \"Compare the commonalities of the risks with the highest total loss across the sites for Dampier Port, Cape Lambert and Oyu Tolgoi.\", # Q29\n",
    "    \"What are the recommendations with the greatest benefit in terms of return of investment and overall estimated cost\", # Q30\n",
    "    \"For Risk Management Systems and Elements: which element or subelement is the strongest for a site, or across sites. Is there a subelement that is impacting the overall score the most? \", # Q31\n",
    "    \"On average, what is the lowest performing Risk Management System across the 3 sites?\", # Q32\n",
    "    \"What is the lowest performing Risk Management System on Dampier Port Operations. Include some insights on why this one scored low, and give some ideas on how to improve this Risk Management System.\", # Q33\n",
    "    \"How much did the Risk Management System survey percentage changed for the current year compared to the previous year for Dampier Port Operations? \", # Q34\n",
    "    \"What Risk Management Sub Elements showed the greatest increase for the current year compared to the previous survey at Cape Lambert Operations? What insights can you gather from these observations? \", # Q35\n",
    "    \"What Risk Management Sub Elements showed a decrease in their scores the current year compared to the previous survey score? Give some ideas to address these shortcomings.\" # Q36\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get questions from xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['List all the Critical Risk Scenarios that impact Cape Lambert',\n",
       " 'Provide me with a brief description of Dampier Port',\n",
       " 'Describe the 2021 Dredging Campaign carried out at Dampier Port',\n",
       " 'Compare the critical risk scenarios of CLO and DPO',\n",
       " 'What are the common recommendations between CLO and DPO?',\n",
       " 'What equipments are being impacted by the risk with the highest total loss at Dampier Port operation',\n",
       " 'What facilities are being impacted by risks at Oyu Tolgoi and  Dampier Port operations',\n",
       " 'What is the total business interuption of top 5 risks in Oyu Tolgoi operation',\n",
       " 'What are the key information about the critical assessment report  conducted for Dampier Port operation',\n",
       " 'what  are the electricity sources of Oyu Tolgoi and  Dampier Port operations',\n",
       " 'what  are the common sources of electricity  between Oyu Tolgoi and  Dampier Port operations',\n",
       " 'What scenarios exist for cascading risks happening at  Dampier Port operation',\n",
       " 'List all equipments with their corresponding critical risk scenrairo at Dampier Port Operation',\n",
       " 'List all facilities with their corresponding critical risk scenrairo at Dampier Port Operation',\n",
       " 'There is a predicted high temprature at Dampier port operation, list all potentianl critical risk scenarios and recommendations associated along their IDs. Explain how these scenarios are linked to the high temperature',\n",
       " 'List all the existing maintenance projects at Dampier port operation',\n",
       " 'is there any common recommendations for critical risk scenarios accross the operations you have in your knowledge base',\n",
       " 'Give me a comparisson of the critical risk scenarios in Dampier Port Operation and Cape Lambert Operation. What similarities can we observe on both operations?',\n",
       " 'Out of DPO and CLO, which operation is more likely to be hit by a cyclone',\n",
       " 'What are the top 5 risks in terms of total loss across the sites for Dampier Port, Cape Lambert and Oyu Tolgoi',\n",
       " 'What is the recommendation that mitigates the most in terms of total loss across the sites for Dampier Port, Cape Lambert and Oyu Tolgoi. ',\n",
       " 'What are the most likely risks across the sites for Dampier Port, Cape Lambert and Oyu Tolgoi. Include the likelihood ranking',\n",
       " 'Give me a list of all the risks with likelihood of \"Possible\" across the sites for Dampier Port, Cape Lambert and Oyu Tolgoi. ',\n",
       " 'What year was the risk Scenario CLO-03 identified?',\n",
       " 'What are the main sources of energy for the power station providing power to Dampier Port',\n",
       " 'What are the pieces of equipement that are found in large critical risk scenarios. ',\n",
       " 'What are the most common types of maintenance being conducted at across the sites in your knowledge base',\n",
       " 'What are the main differences between Dampier Port and Cape Lambert',\n",
       " 'Is there a single point of failure in Cape Lambert in terms of electricity supply?',\n",
       " 'Compare the commonalities of the risks with the highest total loss across the sites for Dampier Port, Cape Lambert and Oyu Tolgoi.',\n",
       " 'What are the recommendations with the greatest benefit in terms of return of investment and overall estimated cost',\n",
       " 'For Risk Management Systems and Elements: which element or subelement is the strongest for a site, or across sites. Is there a subelement that is impacting the overall score the most? ',\n",
       " 'On average, what is the lowest performing Risk Management System across the 3 sites?',\n",
       " 'What is the lowest performing Risk Management System on Dampier Port Operations. Include some insights on why this one scored low, and give some ideas on how to improve this Risk Management System.',\n",
       " 'How much did the Risk Management System survey percentage changed for the current year compared to the previous year for Dampier Port Operations? ',\n",
       " 'What Risk Management Sub Elements showed the greatest increase for the current year compared to the previous survey at Cape Lambert Operations? What insights can you gather from these observations? ',\n",
       " 'What Risk Management Sub Elements showed a decrease in their scores the current year compared to the previous survey score? Give some ideas to address these shortcomings.',\n",
       " '\"For each Risk Management System of Dampier Port, how did the survey percentage change for the current year compared to the previous year? And what is the average percentage change for the current year?\"']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract questions from an xlsx file and convert to a list\n",
    "\n",
    "questions_file_path = os.path.join(\"..\", \"validation\", \"LLM Questions - Workshop 241024.xlsx\")\n",
    "questions = pd.read_excel(questions_file_path, header=0)\n",
    "questions = questions.iloc[:, 1].tolist()\n",
    "\n",
    "questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "sparql_handler = SPARQLQueryHandler(max_sparql_retries=3)\n",
    "vectorstore_handler = VectorStoreHandler(llm, doc_index_name, ontology_index_name, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe to store the results\n",
    "results_df = pd.DataFrame(columns=[\"input_query\", \"generated_sparql\", \"sparql_results\", \"PTO_results\", \"retrieved_docs\", \"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_queries = [\n",
    "\"What are the recommendations with the greatest benefit in terms of return of investment and overall estimated cost\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Querying ontology database:  Recommendations with the greatest benefit in terms of return on investment and overall estimated cost.\n",
      "### Successful sparql query:\n",
      "PREFIX cro: <http://WSP.org/ontology/cro#>\n",
      "SELECT ?recommendation ?recommendationPriority ?recommendationEstimatedCostUSDM\n",
      "WHERE {\n",
      "    ?recommendation cro:recommendationPriority \"High Return\" ;\n",
      "                    cro:recommendationEstimatedCostUSDM ?recommendationEstimatedCostUSDM .\n",
      "}\n",
      "ORDER BY DESC(?recommendationEstimatedCostUSDM)\n",
      "### Querying pdf document database:  What recommendations offer the greatest return on investment and the lowest overall estimated cost?\n",
      "### Generated answer:\n",
      "The recommendations with the greatest benefit in terms of return on investment and overall estimated cost are as follows:\n",
      "\n",
      "1. **CLO-22-10-02**: This recommendation has an estimated benefit of 50.0.\n",
      "2. **DPO-22-08-01**: This recommendation also has an estimated benefit of 50.0.\n",
      "3. **DPO-22-08-02**: This recommendation has an estimated benefit of 30.0.\n",
      "4. **CLO-21-09-03**: This recommendation has an estimated benefit of 20.0.\n",
      "5. **CLO-23-08-03**: This recommendation has an estimated benefit of 10.0.\n",
      "\n",
      "These recommendations are classified as high return priorities and should be responded to within 90 days. The classifications and response times are defined in the Critical Risk Assessment guidelines, which emphasize the importance of timely action to maximize the benefits and manage risks effectively.\n"
     ]
    }
   ],
   "source": [
    "for input_query in input_queries:\n",
    "    \n",
    "    # SPARQL Query Generation\n",
    "    vectorstore_handler.k = 5\n",
    "    ontology_index_results = vectorstore_handler.perform_graph_search(graph_search_prompt, input_query)\n",
    "\n",
    "    useful_individuals = []\n",
    "    for result in ontology_index_results:\n",
    "        useful_individuals.append(result.page_content)\n",
    "    \n",
    "    sparql_generation_chain_result = sparql_generation_chain.invoke(\n",
    "        {\"prompt\": input_query, \"individuals\": str(useful_individuals), \"schema\": non_inst_ontology, \"sparql_queries\": sparql_templates}\n",
    "    )\n",
    "\n",
    "    raw_sparql = sparql_generation_chain_result.content\n",
    "    generated_sparql = sparql_handler.get_prepared_sparql_query(_run_manager, raw_sparql, non_inst_ontology)\n",
    "    sparql_results = sparql_handler.execute_query(g, generated_sparql)\n",
    "        \n",
    "    # If no results, iterate up to a max number of attempts\n",
    "    query_retries = 0\n",
    "    while sparql_results == [] and query_retries < max_query_retries:\n",
    "        \n",
    "        query_retries += 1\n",
    "        \n",
    "        print(f\"### Attempt {query_retries}:\", generated_sparql)\n",
    "        print(f\"### No results retrieved by the query, generating a new one...\")\n",
    "\n",
    "        # Use no_result_chain to generate a new query\n",
    "        no_result_chain_result = no_result_chain.invoke(\n",
    "            {\n",
    "                \"generated_sparql\": generated_sparql,\n",
    "                \"prompt\": input_query,\n",
    "                \"schema\": non_inst_ontology,\n",
    "                \"individuals\": str(useful_individuals),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Get the newly generated SPARQL query and execute it\n",
    "        generated_sparql = no_result_chain_result.content\n",
    "        generated_sparql = sparql_handler.get_prepared_sparql_query(_run_manager, generated_sparql, non_inst_ontology)\n",
    "        sparql_results = sparql_handler.execute_query(g, generated_sparql)\n",
    "\n",
    "    # Handle case when no results were retrieved after max iterations\n",
    "    if sparql_results == []:\n",
    "        print(f\"### No results from graph database after {max_query_retries} attempts for query: '{input_query}'. Falling back to standard RAG approach.\")\n",
    "    else:\n",
    "        print(\"### Successful sparql query:\")\n",
    "        print(generated_sparql)\n",
    "\n",
    "    vectorstore_handler.k = 5\n",
    "    # Vector store search\n",
    "    pdf_index_results = vectorstore_handler.perform_vector_search(pdf_search_prompt, input_query)\n",
    "    \n",
    "    # Answer generation\n",
    "    try:\n",
    "        combined_chain_result = combined_chain.invoke(\n",
    "            {\n",
    "                \"question\": input_query, \n",
    "                \"sparql_results\": sparql_results,  \n",
    "                \"index_results\": pdf_index_results\n",
    "            }\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred while generating the answer. Falling back to standard RAG.\")\n",
    "        print(e)\n",
    "        combined_chain_result = combined_chain.invoke(\n",
    "            {\n",
    "                \"question\": input_query, \n",
    "                \"sparql_results\": \"### The output of the graph query included too many results and exceeded the context window size.\",  \n",
    "                \"index_results\": pdf_index_results\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    result = combined_chain_result.content\n",
    "    print(\"### Generated answer:\")\n",
    "    print(result)\n",
    "\n",
    "    # Store results\n",
    "    test_result = pd.DataFrame([{\n",
    "        \"input_query\": str(input_query),\n",
    "        \"generated_sparql\": str(generated_sparql),\n",
    "        \"sparql_results\": str(sparql_results),\n",
    "        \"PTO_results\": str(ontology_index_results),\n",
    "        \"retrieved_docs\": str(pdf_index_results),\n",
    "        \"result\": str(result),\n",
    "    }])\n",
    "\n",
    "    results_df = pd.concat([results_df, test_result], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(rdflib.term.Literal('Flooding of car dumper 5 vault lower levels has previously been an issue. Reports of two instances in 2013 and 2014 of significant flooding of the CD5C cutting and vault gave rise to concerns of the adequacy and reliability of the stormwater pumping system (inclusive of its electricity supply and backup). This has now been addressed with the advent of a more suitable pumping arrangement. The flooding also occurred in 2019 during a cyclone when the site was evacuated. Plant control logic has been changed to always keep the submersible pump on during a cyclone when the site has been evacuated. And were used effectively in Cyclone Damian and can now be accessed remotely.'),),\n",
       " (rdflib.term.Literal('The provision of infrastructure (electricity and water) at CLB is of a reasonable standard. More work is underway. Massive water leaks have been detected. Two separate projects are underway to resolve the water leak issue at CLA and CLB. Multiple failure modes identified. A Water superintendent is now on site and two water infrastructure upgrades have occurred since last year. MIC prevention by replacing steel with poly pipe and Tank reline occurred on CLB water tank Q4 2020. However the electricity supply system is extremely robust, with two \"rings\" of main supplies to the plant substations (i.e. each substation has dual redundant feeders, all substations can be maintained operational even with a single failure in the ring of supply cables).'),),\n",
       " (rdflib.term.Literal('Very significant improvements in the provision of the electricity supply to the site have occurred in the last years, with a new 220 kV transmission line from Yurralyi Maya Power Station near Karratha and a new bulk substation supplying CLA and CLB. An 80 MWe open cycle gas turbine power station has been commissioned at Cape Lambert (2017) to further reinforce the site power supply arrangements.'),)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparql_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':DPO_Vehicle_Interaction is a NamedIndividual , :RiskManagementSubElement ; :currentSurveyScore 8 ; :previousSurveyScore 8 ; :riskManagementSubElementInformation \"Risk Management SubElement Information\"; \"Critical controls for the Vehicle Interaction Management Plan are audited internally for compliance. Opportunities for improvement are reviewed on a quarterly basis or more frequently. DPO uses CRM for this task. CONFIDENTIALHamersley Iron Pty Ltd - Dampier Port Rio Tinto – Critical Risk Assessment 16 October 2023 Page 73\" , \"The site has conducted a risk assessment to identify the potential root causes of collisions and rollovers at the site. Control strategies have been developed to mitigate the risk of Vehicle Interaction.\" , \"The site has developed a Management Plan or Procedure to provide a framework for effective management of Vehicle Interaction activities, to minimise any harm to persons, property, the environment, reputation or financial loss to the business.\" , \"The site has well established procedures for bulking in/out. Roads and intersections are signed and marked. Road speed limits apply. Only authorised drivers who are trained and deemed competent are permitted to drive on the site. A secure boom gate prevents unauthorised access to the site. Heavy and light vehicle interaction points are controlled.\" , \"This assessment item is primarily directed at mining operations having heavy mobile equipment (and their interactions with other heavy and light vehicles and haul road safety). This is applicable for the aspect of \\\\\"bulking in & out\\\\\" using heavy mobile equipment for the moving of iron ore within the port stockyards.\" ; rdfs:label \"DPO_Vehicle_Interaction\" .',\n",
       " ':DPO_Hydrants_and_Fire_Water_Supplies is a NamedIndividual , :RiskManagementSubElement ; :currentSurveyScore 5 ; :previousSurveyScore 5 ; :riskManagementSubElementInformation \"Risk Management SubElement Information\"; \"DPO-21-09-01 CONFIDENTIALHamersley Iron Pty Ltd - Dampier Port Rio Tinto – Critical Risk Assessment 16 October 2023 Page 85\" , \"DPO-23-08-06\" , \"Fire hydrant outlets are connected to the combined water system. With respect to the fire hydrant network serving site, the following is noted: - Fire hydrants at PPt wharf are located at approximately 30 metre intervals. - Fire hydrants at the EII wharf have been relocated from under the conveyor to locations accessible by attending ERT. - A low height fire tender provides improved access for the ERT during periods when the ship loader may obstruct the main fire appliance. The identified fire appliance has the ability to travel out on the wharf and under the shiploader boom when it is at its lowest position.\" , \"Owing to poor performance of the fire hydrant system, a recommendation has been made for site to investigate issues with the water supply to identify what rectification is necessary to achieve acceptable performance.\" , \"Previous recommendations have been made to upgrade the fire water supplies at both sites, with recent upgrades to the system including the refurbishment of the EII Car Dumper water tank (Q3 2021). Additional upgrade works include the refurbishment of the PPt water tank and the EII Causeway water tank, which are scheduled to be completed by Q4 2023 and Q1 2025 respectively. It is noted that during the refurbishment of the EII Causeway water tank, which is scheduled to commence Q4 2023, two temporary 1.5ML water tanks are to be implemented. However, only a single tank is to contain a reserve for fire water (reserve capacity of 0.5ML).\" , \"Site advise that flow meters have been installed around stockyards to monitor and balance water supplies.\" , \"The EII causeway belt conveyor has been identified to contain a significant shortfall in fire hydrant coverage along the length of the conveyor (in excess of 1km). A recommendation has been made for site to review the hydrant coverage to the EII causeway and, where necessary, install additional hydrants in accordance with the relevant standards of performance and RTGRF Guidance Notes.\" , \"The Fuel Farm fire protection water supply systems comprise of a dedicated fire water booster connection, fire water storage tanks, fire pumps, together with fire hydrants and foam monitors. Foam / water cannons are located external to the bund wall and are located well within a 20 metre radius of bulk storage tanks. Site specific scenarios have been prepared together with ERT drills for which exercises are conducted annually for the support of the revised fuel farm storage facilities and fire fighting strategies.\" , \"The water supply to the Dampier area is drawn from two Western Australian Water Authority (WAWA) 912cu.m gravity tanks that are fed from the public Mill Stream / Hardy River Dam supply. Isolation valves have been provided across the site to permit sectional control. The water supply systems for Parker Point (PPt) and East Intercourse Island (EII) are provided as follows: - The Parker Point (PPt) water supply system is reticulated for plant and fire protection use and includes associated general works area. The supplies are looped at the stockyard and port facilities. The water supply for fire protection services at PPt is provided from a common site main (process / fire water). This system is supplied via three domestic electric pumps and is supported by an automatic start dedicated diesel fire pump. The pumps draw from a ground level steel tank (1,000cu.m capacity - 300cu.m reserved for fire water), which is infilled from the aforementioned 912cu.m gravity tanks. - The water supply to East Intercourse Island (EII) is drawn from a metered connection to the WAWA supply (i.e. the connection taps into the overland line supplying the two 912cu.m WAWA tanks). The EII water supply provides both process and fire water. The network is configured such that only a small portion of the stockyard is looped. The system is boosted in part by a single combined process / fire diesel pump located at both the Car Dumper and at the Causeway. The EII Car Dumper has an automatic start combined process / fire diesel fire pump, which takes suction from a 455cu.m tank. At the Causeway there are three domestic electric pumps, which are supported by an automatic start combined process / fire diesel pump, which takes suction from a 9,613cu.m tank (2,043cu.m reserved for fire water). CONFIDENTIALHamersley Iron Pty Ltd - Dampier Port Rio Tinto – Critical Risk Assessment 16 October 2023 Page 84\" , \"While the fuel farm fire water system installation is independent of the port fire systems, it relies upon the adequacy of the Parker Point water supply to recharge tanks and the site water system as a connection point for fire appliance booster equipment. Please refer to the following recommendations:\" ; rdfs:label \"DPO_Hydrants_&_Fire_Water_Supplies\" .',\n",
       " ':clo_assumptions_and_basis_2023 is a NamedIndividual , :AssumptionsAndBasis ; :assumptionDescription \"Description of the assumption\"; \"All RTIO ore is railed and delivered to the ports in either \\'pooled fleet\\' ore cars or from the Robe Valley in dedicated \\'Robe Valley\\' ore cars (only railed to the car dumper CD1C at CLA). The forecast total ore from both ports to be shipped in 2023, which is delivered in the pooled fleet ore cars is 340.0 Mt less the planned 30.0 Mt Robe Valley ore, or 310.0 Mt. It should be noted that there are only three consists of Robe fleet ore cars left and pool fleet consists are also carting robe valley ore to car dumpers 6 & 7 and using the cross link to the CLA terminal for processing.\" , \"Based on Asset Reinstatement Planning (ARP) work undertaken in 2006, the average salvage time of 10 and 12 weeks has been assumed for scenarios involving ship sinking at berth (Critical Risk Scenarios).\" , \"In all cases of the spare capacity at the CLA, CLB and Dampier ports, it is assumed the capacity is available immediately (subject in practical terms to a delay of 12 to 24 hours related to rescheduling and revising production and railing plans. Note: This small delay is ignored in this report for the sake of simplicity in the loss calculations).\" , \"Losses involving the CLA car dumper CD1C circuit (Robe Valley ore) is now offset by the completion of the link conveyor route from the CLB car dumpers CD6C and CD7C to feed to the CLA process plant coarse ore stockpile. The mitigations have been applied to relevant Critical Risk Scenarios in this report. Note: Using these car dumpers involves the loading of pooled fleet cars at the Robe Valley mines.\" , \"Maximum capacities of the car dumpers are as follows: CD2E - 49.2 Mtpa. CD4P - 64 Mtpa. CD3P - 64 Mtpa. CD1C - 37 Mtpa. CD2C - 58 Mtpa. CD5C - 62 Mtpa. CD6C - 62 Mtpa. CD7C - 62 Mtpa.\" , \"The Business Interruption values for the Cape Lambert Operations are determined from information supplied by the Business Unit as at 1 March 2023. The estimated declared values are USD17,761.6M per annum. Any subsequent changes in commodity prices have not been considered in determining the loss estimates and the actual production/shipment may vary from planned/forecast volumes due to market condition.\" , \"The combined Dampier port operations nominal capacity of 159.2Mtpa exceeds its 2023 forecast shipped tonnes of 150.0 Mt, which has been assumed to provide immediately available capacity of 9.2 Mtpa to reduce the net impact of lost shipped tonnes at either CLA (for losses affecting pooled fleet dumping only) or CLB.\" , \"The cost of mobile equipment is assumed to be USD2.0/tonne to bulk in and out of stockpiles.\" , \"The declared Business Interruption (BI) value for the Cape Lambert port of USD $17,716.6M, comprises USD7,521.9M of ore shipped from the Cape Lambert A (CLA) port terminal and USD10,239.7M of ore shipped from the Cape Lambert B (CLB) port terminal.\" , \"The loss calculations are based on the assumption that the RTIO shiploader maximum capacities (available in the case that an event has affected another port, leading to no rail constraints on ore delivered to the port) as follows: SL1E (at EII): 49.2 Mtpa (limited to the CD2E inloading capacity). SL2P and SL3P (at PPt): 55 Mtpa. SL2C (CLA): 46 Mtpa. SL3C (CLA): 48 Mtpa. SL11C and SL12C (at CLB): 58.5 Mtpa.\" , \"The nominal capacity of the Cape Lambert port operation is assumed to be 209.0 Mtpa. This is a combination of 85.0 Mtpa at CLA and 124.0 Mtpa at CLB. It assumed that the CLA port terminal therefore can provide immediately available capacity of 5.0 Mtpa to reduce the net impact of losses occurring at CLB or the Dampier ports. Note: This excess capacity is only available for pooled fleet ore cars via the CLB car dumpers CD6C and CD7C inloading to the CLA stockyard. It is also assumed that the CLB port terminal therefore can provide 14.0 Mtpa immediately available capacity to reduce the net impact of losses occurring at CLA or the Dampier ports.\" , \"The planned throughput for the Cape Lambert port in 2023 is 190 Mt out of a total of 350.0 Mt for the two RTIO ports, with the remainder, 150.0 Mt being shipped from the Dampier port. The Cape Lambert shipping forecast comprises 80.0 Mt from the CLA terminal (including ~30.0 Mt of Robe Valley ore), and the remainder, 110.0 Mt, from the CLB terminal. These are the figures used in the Critical Risk Scenarios.\" , \"The weekly value of Business Interruption is calculated to be USD341.57M.\" .',\n",
       " ':CLO_Production_Levels_and_Plant_Redundancy is a NamedIndividual , :RiskManagementSubElement ; :currentSurveyScore 8 ; :previousSurveyScore 8 ; :riskManagementSubElementInformation \"Risk Management SubElement Information\"; \"In 2023 there exists excess capacity over the planned shipments of iron ore from both Dampier and Cape Lambert. This capacity is able to offset some of the loss events affecting the port. CONFIDENTIALRobe River Mining Company Pty Ltd - Cape Lambert Operations Rio Tinto – Critical Risk Assessment 16 October 2023 Page 79\" , \"The production plan has a reasonable allowance for unplanned production delays and current production levels are close to, or better than plan. This is due to the supply chain being restrained by rail capacity not port capacity.\" , \"There are stockpiles in the supply chain that can be drawn down to maintain sales for most foreseeable failures - stockpiles are located at EII, PPt and Cape Lambert as well as at the individual mine operations.\" ; rdfs:label \"CLO_Production_Levels_&_Plant_Redundancy\" .',\n",
       " ':DPO-01 is a NamedIndividual , :CriticalRiskScenario ; :impactsOperation :dpo_dampier_port ; :riskImpactsFacility :dpo_east_intercourse_island , :dpo_parker_point ; :BISummary \"The reported annual business interruption value is USD13,993.40M. Therefore: Loss of all Dampier port shipping: (USD13,993.40M x 4.0 weeks/52 weeks x 100%) = USD1,076.42M.\" ; :assetGroup \"Port Facilities\" ; :businessInterruptionUSD 1.0 ; :hardControls \"Hard Controls\"; :incidentDescription \"Description of the incident\"; :incidentSummary \"Cyclone damages facilities, infrastructure and townships\" ; :inherentControls \"Inherent Controls\"; :inherentHazards \"Inherent Hazards\"; :likelihood \"Unlikely\" ; :lossScenarioPeril \"Climatic Asset Group Port Facilities\" ; :preincident_and_workaroundARP \"\\uf0b7 Location specific and regional Business Resilience and Recovery Teams are established,\" ; :propertyDamageUSD 306.0 ; :recoveryRepair \"The widespread damage to local community infrastructure is given priority by RTIO / Local / State and Federal Government for repairs and resources. This delays inspection and repairs at the port operations and related infrastructure. Repair and recovery would be focused on returning the Berth structures and Shiploader to service. Total repairs of USD 306M.\" ; :repairPeriod \"Repair Period\"; :softControls \"Soft Controls\"; :workaround \"Workaround\"; :hazardsAndControls \"Hazards and Controls\"; :totalLossUSD 1.0 .']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most likely risks across the sites for Dampier Port, Cape Lambert, and Oyu Tolgoi, along with their likelihood rankings, are as follows:  ###\n",
      "Dampier Port 1. **DPO-04**: Rare 2. **DPO-07**: Rare 3. **DPO-12**: Rare 4. **DPO-23**: Rare 5. **DPO-01**: Unlikely 6. **DPO-05**: Unlikely 7.\n",
      "**DPO-10**: Unlikely 8. **DPO-11**: Unlikely 9. **DPO-16**: Unlikely 10. **DPO-20**: Unlikely 11. **DPO-22**: Unlikely  ### Cape Lambert Operations 1.\n",
      "**CLO-02**: Rare 2. **CLO-05**: Rare 3. **CLO-07**: Rare 4. **CLO-10**: Rare 5. **CLO-12**: Rare 6. **CLO-23**: Rare 7. **CLO-24**: Rare 8.\n",
      "**CLO-03**: Unlikely 9. **CLO-04**: Unlikely 10. **CLO-06**: Unlikely 11. **CLO-09**: Unlikely 12. **CLO-11**: Unlikely 13. **CLO-13**: Unlikely 14.\n",
      "**CLO-22**: Unlikely  ### Oyu Tolgoi There is no information available regarding the risks and their likelihood rankings for Oyu Tolgoi.  For\n",
      "additional context, the risks at Dampier Port and Cape Lambert include potential damage from cyclones, flooding, and structural failures. These events\n",
      "can lead to significant operational disruptions and financial losses.\n"
     ]
    }
   ],
   "source": [
    "print(textwrap.fill(result, width=150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate question codes\n",
    "code_sequence = [f\"CRD-{str(i).zfill(2)}\" for i in range(1, len(results_df) + 1)]\n",
    "try:\n",
    "    results_df.insert(0, 'code', code_sequence)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'Outcome' and 'Validation' columns\n",
    "results_df['outcome'] = results_df['sparql_results'].apply(lambda x: 0 if x == \"[]\" else 1)\n",
    "positive_outcome_count = results_df['outcome'].sum()\n",
    "total_outcome_count = len(results_df)\n",
    "results_df['validation'] = ((positive_outcome_count / total_outcome_count) * 100).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics\n",
    "validation_percentage = (positive_outcome_count / total_outcome_count) * 100\n",
    "failed_questions_df = results_df[results_df['outcome'] == 0]\n",
    "failed_question_count = len(failed_questions_df)\n",
    "failed_question_codes = failed_questions_df['code'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data to Excel\n",
    "timestamp = pd.Timestamp.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "output_file_path = os.path.join(\"..\", f\"validation/validation_results/Combined RAG Validation_{timestamp}.xlsx\")\n",
    "\n",
    "with pd.ExcelWriter(output_file_path, engine='xlsxwriter') as writer:\n",
    "\n",
    "    # Save results data to Sheet1\n",
    "    results_df.to_excel(writer, sheet_name='Sheet1', index=False)\n",
    "\n",
    "    # Create summary data for Sheet2\n",
    "    summary_data = {\n",
    "        'Statistic': ['Accuracy', 'Error Count', 'Error Codes'],\n",
    "        'Value': [f'{validation_percentage:.2f}%', failed_question_count, ', '.join(str(failed_question_codes))]\n",
    "    }\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "    summary_df.to_excel(writer, sheet_name='Sheet2', index=False)\n",
    "\n",
    "    # Access the workbook and worksheet for formatting\n",
    "    workbook = writer.book\n",
    "    worksheet1 = writer.sheets['Sheet1']\n",
    "    worksheet2 = writer.sheets['Sheet2']\n",
    "\n",
    "    # Set column width\n",
    "    worksheet1.set_column(0, len(results_df.columns) - 1, 30)\n",
    "    worksheet2.set_column(0, 1, 20)\n",
    "\n",
    "    # Add conditional formatting to the outcome column in Sheet1\n",
    "    outcome_range = f'G2:F{len(results_df) + 1}'\n",
    "    format_pass = workbook.add_format({'bg_color': '#77DD77'})\n",
    "    format_fail = workbook.add_format({'bg_color': '#FF0000'})\n",
    "\n",
    "    worksheet1.conditional_format(outcome_range, {\n",
    "        'type': 'cell',\n",
    "        'criteria': '==',\n",
    "        'value': 1,\n",
    "        'format': format_pass\n",
    "    })\n",
    "\n",
    "    worksheet1.conditional_format(outcome_range, {\n",
    "        'type': 'cell',\n",
    "        'criteria': '==',\n",
    "        'value': 0,\n",
    "        'format': format_fail\n",
    "    })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
